{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747094102002
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#from azure.ai.ml import MLClient, Input, MpiDistribution, command\n",
        "from azure.ai.ml import MLClient, Input, Output, PyTorchDistribution, command\n",
        "from azure.ai.ml.entities import AmlCompute, Environment, BuildContext, Data\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Azure ML workspace configuration\n",
        "SUBSCRIPTION_ID = os.getenv(\"SUBSCRIPTION_ID\")\n",
        "RESOURCE_GROUP = os.getenv(\"RESOURCE_GROUP\")\n",
        "WORKSPACE_NAME = os.getenv(\"WORKSPACE_NAME\")\n",
        "COMPUTE_CLUSTER = \"demo-gpucluster01\"\n",
        "# Wandb Settings\n",
        "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
        "WANDB_ENTITY = os.getenv(\"WANDB_ENTITY\")\n",
        "\n",
        "# authentication via managed identity or service principal (no hard-coded creds)\n",
        "ml_client = MLClient(DefaultAzureCredential(), SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)\n",
        "\n",
        "# ensure compute cluster exists or create it\n",
        "try:\n",
        "    ml_client.compute.get(COMPUTE_CLUSTER)\n",
        "except Exception:\n",
        "    print(\"demo-gpucluster01 was not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Docker environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747094295377
        }
      },
      "outputs": [],
      "source": [
        "CLOUD_DIR = \"./azureml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {CLOUD_DIR}/train/Dockerfile\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu121-py310-torch22x:biweekly.202504.1\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "\n",
        "# Inference requirements\n",
        "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20230419.v1 /artifacts /var/\n",
        "RUN /var/requirements/install_system_requirements.sh && \\\n",
        "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
        "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
        "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
        "    rm -f /etc/nginx/sites-enabled/default\n",
        "ENV SVDIR=/var/runit\n",
        "ENV WORKER_TIMEOUT=400\n",
        "EXPOSE 5001 8883 8888\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y openssh-server openssh-client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {CLOUD_DIR}/train/requirements.txt\n",
        "azureml-core==1.60.0\n",
        "azureml-dataset-runtime==1.60.0\n",
        "azureml-defaults==1.60.0\n",
        "azure-ml==0.0.1\n",
        "azure-ml-component==0.9.18.post2\n",
        "azureml-mlflow==1.60.0\n",
        "azureml-contrib-services==1.60.0\n",
        "azureml-contrib-services==1.60.0\n",
        "torch-tb-profiler~=0.4.0\n",
        "azureml-inference-server-http\n",
        "inference-schema\n",
        "MarkupSafe==2.1.2\n",
        "regex\n",
        "pybind11\n",
        "urllib3>=1.26.18\n",
        "cryptography>=42.0.4\n",
        "aiohttp>=3.8.5\n",
        "py-spy==0.3.12\n",
        "debugpy~=1.6.3\n",
        "ipykernel~=6.0\n",
        "tensorboard\n",
        "psutil~=5.8.0\n",
        "matplotlib~=3.5.0\n",
        "tqdm~=4.66.3\n",
        "py-cpuinfo==5.0.0\n",
        "torch-tb-profiler~=0.4.0\n",
        "# huggingface\n",
        "transformers>=4.36.0\n",
        "datasets\n",
        "accelerate\n",
        "optimum\n",
        "peft\n",
        "appdirs\n",
        "loralib\n",
        "scipy\n",
        "py7zr  # 圧縮解凍library\n",
        "bitsandbytes\n",
        "fire  # argparser\n",
        "# formatter & linter\n",
        "black\n",
        "flake8\n",
        "# tokenizer\n",
        "sentencepiece\n",
        "# logging\n",
        "wandb\n",
        "# multi node\n",
        "mpi4py\n",
        "# megatron-lm\n",
        "nltk\n",
        "pybind11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746495240360
        }
      },
      "outputs": [],
      "source": [
        "env_name = \"llama3-8b-wiki_env\"\n",
        "docker_dir=f\"{CLOUD_DIR}/train\"\n",
        "\n",
        "env_docker_image = Environment(\n",
        "    build=BuildContext(path=docker_dir),\n",
        "    name=env_name,\n",
        "    description=\"Environment created from a Docker context.\",\n",
        ")\n",
        "env_asset = ml_client.environments.create_or_update(env_docker_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5> Register the training dataset </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747102962354
        }
      },
      "outputs": [],
      "source": [
        "data = Data(\n",
        "    path=\"azureml://datastores/workspaceblobstore/paths/wiki-indexed-dataset1/\",\n",
        "    type = AssetTypes.URI_FOLDER,\n",
        "    description = \"wiki dump data for pretraining\",\n",
        "    name = \"wiki_dump_01\",\n",
        "    version = '1'\n",
        ")\n",
        "\n",
        "ml_client.data.create_or_update(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747103264194
        }
      },
      "outputs": [],
      "source": [
        "# job configuration\n",
        "NUM_NODES = 2\n",
        "NUM_GPU_PER_NODE = 1\n",
        "CACHE_DIR = \"${{outputs.cache}}\"\n",
        "\n",
        "dist = PyTorchDistribution(\n",
        "    process_count_per_instance=NUM_GPU_PER_NODE,\n",
        "    node_count=NUM_NODES\n",
        ")\n",
        "\n",
        "job = command(\n",
        "    code=\"./azureml\",\n",
        "    command=(\n",
        "        # C++ extension build\n",
        "        \"rm -f megatron_lm/megatron/core/datasets/helpers_cpp*.so && \"\n",
        "        \"make -C megatron_lm/megatron/core/datasets && \"\n",
        "        f\"mkdir -p {CACHE_DIR} && \"\n",
        "        # Run training\n",
        "        f\"python examples/finetuning.py \\\n",
        "        --fsdp-cpu-offload \\\n",
        "        --fsdp-activation-checkpointing \\\n",
        "        --low-cpu-fsdp \\\n",
        "        --bf16 \\\n",
        "        --epoch 3 \\\n",
        "        --base-model ${{inputs.model_dir}} \\\n",
        "        --tokenizer-type Llama2Tokenizer \\\n",
        "        --tokenizer-model ${{inputs.model_dir}} \\\n",
        "        --global-batch-size 128 \\\n",
        "        --micro-batch-size 8 \\\n",
        "        --min-lr 1e-5 \\\n",
        "        --lr 1e-4 \\\n",
        "        --lr-warmup-iters 0 \\\n",
        "        --lr-decay-style cosine \\\n",
        "        --train-iters 8 \\\n",
        "        --lr-decay-iters 1 \\\n",
        "        --weight-decay 0.1 \\\n",
        "        --train-data-path ${{inputs.train_data}}/wikidump.jsonl \\\n",
        "        --data-cache-path {CACHE_DIR}  \\\n",
        "        --seq-length 4096 \\\n",
        "        --sliding-window-size 4096 \\\n",
        "        --num-workers 2 \\\n",
        "        --save-interval 100 \\\n",
        "        --save ./outputs/checkpoints \\\n",
        "        --load ./outputs/checkpoints \\\n",
        "        --use-better-transformer \\\n",
        "        --wandb-entity {WANDB_ENTITY} \\\n",
        "        --wandb-project llama3-8b-test \\\n",
        "        --wandb-name llama3-8b-wiki_dataset\"\n",
        "    ),\n",
        "    inputs={\n",
        "        \"train_data\": Input(\n",
        "            type=AssetTypes.URI_FOLDER, \n",
        "            path=\"wiki_dump_01@latest\",\n",
        "            mode=\"ro_mount\"\n",
        "        ),\n",
        "        \"model_dir\": Input(\n",
        "            type=AssetTypes.URI_FOLDER, \n",
        "            path=\"llama3-8b@latest\",\n",
        "            mode=\"ro_mount\"\n",
        "        )\n",
        "    },\n",
        "    outputs={\n",
        "        \"cache\": Output(\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            mode=\"rw_mount\" \n",
        "        )\n",
        "    },\n",
        "    environment=\"llama3-8b-wiki_env@latest\",\n",
        "    compute=COMPUTE_CLUSTER,\n",
        "    instance_count=NUM_NODES,\n",
        "    distribution=dist,\n",
        "    environment_variables={\n",
        "        \"MEGATRON_CACHE\" : CACHE_DIR, \n",
        "        \"NCCL_IB_DISABLE\": \"1\", \n",
        "        \"LOGLEVEL\": \"INFO\",\n",
        "        \"NCCL_DEBUG\": \"INFO\",\n",
        "        \"NCCL_DEBUG_SUBSYS\": \"WARN\",\n",
        "        \"PYTHONFAULTHANDLER\": \"1\",\n",
        "        \"CUDA_LAUNCH_BLOCKING\": \"0\",\n",
        "        \"WANDB_MODE\": \"online\",\n",
        "        \"WANDB_API_KEY\": WANDB_API_KEY\n",
        "    },\n",
        "    display_name=\"llama3-8b-wiki-pretrain\",\n",
        "    experiment_name=\"llama3-8b-wiki-exp\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747103674005
        }
      },
      "outputs": [],
      "source": [
        "# submit the job\n",
        "returned_job = ml_client.jobs.create_or_update(job)\n",
        "print(f\"Job submitted: {returned_job.name}\")\n",
        "print(f\"Monitor at: {returned_job.studio_url}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
