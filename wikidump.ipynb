{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://dumps.wikimedia.org/jawiki/20250420/\n",
        "<br>\n",
        "https://dumps.wikimedia.org/jawiki/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746320511484
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url_index = \"https://dumps.wikimedia.org/jawiki/20250420/jawiki-20250420-pages-articles-multistream-index.txt.bz2\"\n",
        "url_xml = \"https://dumps.wikimedia.org/jawiki/20250420/jawiki-20250420-pages-articles-multistream.xml.bz2\"\n",
        "urls = [url_index, url_xml]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5> Download wikidump </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1745802618011
        }
      },
      "outputs": [],
      "source": [
        "for url in urls:\n",
        "    output_file = url.split(\"/\")[-1]\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_file, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "        print(f\"File downloaded successfully: {output_file}\")\n",
        "    else:\n",
        "        print(f\"Failed to download file. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5> Extract Index file </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bunzip2 jawiki-20250420-pages-articles-multistream-index.txt.bz2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5> Peek the contents </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!grep Category jawiki-20250420-pages-articles-multistream-index.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5> Extract blocks </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1745817236636
        }
      },
      "outputs": [],
      "source": [
        "import bz2\n",
        "f = open(\"jawiki-20250420-pages-articles-multistream.xml.bz2\", \"rb\")\n",
        "f.seek(361332488)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1745817284596
        }
      },
      "outputs": [],
      "source": [
        "block = f.read(361635009 - 361332488)\n",
        "data = bz2.decompress(block)\n",
        "xml = data.decode(encoding=\"utf-8\")\n",
        "print(len(xml))\n",
        "print(xml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1745817316508
        }
      },
      "outputs": [],
      "source": [
        "with open(\"./wikidump/wikidump.xml\", \"w\", encoding=\"utf-8\") as xml_file:\n",
        "    xml_file.write(xml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5> Extract TEXT from XML file </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "with open(\"./wikidump/wikidump.xml\", \"rb\") as xml_file:\n",
        "    xml = xml_file.read().decode(\"utf-8\")\n",
        "\n",
        "root = ET.fromstring(\"<root>\" + xml + \"</root>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"./wikidump/wikidump.jsonl\", \"w\", encoding=\"utf-8\") as jsonl_file:\n",
        "    for page in root.findall(\"page\"):\n",
        "        revision = page.find(\"revision\")\n",
        "        if revision is not None:\n",
        "            text = revision.find(\"text\")\n",
        "            if text is not None and text.text is not None:\n",
        "                jsonl_file.write(json.dumps({\"text\": text.text}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"JSONL file 'wikidump2.jsonl' has been created.\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "py12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
